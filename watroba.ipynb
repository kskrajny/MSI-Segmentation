{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kskrajny/MSI-Segmentation/blob/master/watroba.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m_DiStjf09TH",
        "outputId": "19416aad-6976-4b3c-f634-b3079915f9f9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pyspark\n",
            "  Downloading pyspark-3.5.1.tar.gz (317.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m317.0/317.0 MB\u001b[0m \u001b[31m3.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: py4j==0.10.9.7 in /usr/local/lib/python3.10/dist-packages (from pyspark) (0.10.9.7)\n",
            "Building wheels for collected packages: pyspark\n",
            "  Building wheel for pyspark (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pyspark: filename=pyspark-3.5.1-py2.py3-none-any.whl size=317488490 sha256=e3f56117bf2a6f16baf713137c36df71ea44a1f393f05a217fb2792af6257e2a\n",
            "  Stored in directory: /root/.cache/pip/wheels/80/1d/60/2c256ed38dddce2fdd93be545214a63e02fbd8d74fb0b7f3a6\n",
            "Successfully built pyspark\n",
            "Installing collected packages: pyspark\n",
            "Successfully installed pyspark-3.5.1\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (2.0.3)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas) (2024.1)\n",
            "Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas) (2024.1)\n",
            "Requirement already satisfied: numpy>=1.21.0 in /usr/local/lib/python3.10/dist-packages (from pandas) (1.25.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n"
          ]
        }
      ],
      "source": [
        "# Install dependencies\n",
        "!pip install pyspark\n",
        "!pip install pandas\n",
        "\n",
        "# Import libraries\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "from torch import nn\n",
        "from torch.optim import Adam\n",
        "import torch.nn.functional as F\n",
        "from torch.optim.lr_scheduler import CosineAnnealingLR\n",
        "from pyspark.sql import SparkSession\n",
        "from pyspark.ml.linalg import Vectors\n",
        "import os\n",
        "import time\n",
        "\n",
        "start_time = time.time()\n",
        "\n",
        "# Set up Spark session\n",
        "spark = SparkSession.builder \\\n",
        "    .master(\"local\") \\\n",
        "    .appName(\"Magisterka\") \\\n",
        "    .config(\"spark.driver.memory\", \"50g\") \\\n",
        "    .config(\"spark.driver.maxResultSize\", \"0\") \\\n",
        "    .getOrCreate()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "ORfClzfrKhOT"
      },
      "outputs": [],
      "source": [
        "from datetime import datetime\n",
        "import pytz\n",
        "\n",
        "poland_tz = pytz.timezone('Europe/Warsaw')\n",
        "current_time_poland = datetime.now(poland_tz)\n",
        "formatted_date = current_time_poland.strftime('%d-%m-%Y-%H-%M')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "Wcp1pemj3WeX",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b34893cb-6eae-463f-b5c2-2cc294ab5d61"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "dataset = 'watroba'\n",
        "\n",
        "if dataset == \"watroba\":\n",
        "  input_dim = 17002\n",
        "  last_dim = 64\n",
        "  kernel_sizes = [13, 9, 5 ,7 ,7]\n",
        "  hidden_dims = [1, 2, 4, 8, 16, 32]\n",
        "  strides = [3, 3, 3, 3, 3]\n",
        "\n",
        "convolve = \"False\"\n",
        "\n",
        "parquet_dir = \"/content/drive/My Drive/MSI-Segmentation/dane/\" + dataset + \"/parquet_convolve_\" + convolve\n",
        "output_folder = \"/content/drive/My Drive/MSI-Segmentation/results/\" + dataset + f\"_{formatted_date}_conv_\" + convolve\n",
        "\n",
        "class HParams:\n",
        "    def __init__(self, input_dim, kernel_sizes, last_dim, hidden_dims, strides, batch_size=64, lr=1e-3, epochs=1000, batch_step=9000, patience=30):\n",
        "        self.input_dim = input_dim\n",
        "        self.kernel_sizes = kernel_sizes\n",
        "        self.last_dim = last_dim\n",
        "        self.hidden_dims = hidden_dims\n",
        "        self.device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "        self.batch_size = batch_size\n",
        "        self.lr = lr\n",
        "        self.epochs = epochs\n",
        "        self.batch_step = batch_step\n",
        "        self.patience = patience\n",
        "        self.strides = strides\n",
        "\n",
        "hparams_CLR = HParams(input_dim, kernel_sizes, last_dim, hidden_dims, strides)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "DEneQWLh3EZ5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 140
        },
        "outputId": "fdf6f77c-c209-498f-c8bb-30151ddc9127"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"\\n\\nimport torch\\nfrom torch.utils.data import Dataset, DataLoader\\nfrom pyspark.sql import SparkSession\\nimport numpy as np\\n\\nclass ParquetDataset(Dataset):\\n    def __init__(self, parquet_dir, feature_stds, feature_means):\\n        self.spark = SparkSession.builder.getOrCreate()\\n        self.df = self.spark.read.parquet(parquet_dir).sort('pos')\\n        self.data = self.df.collect()\\n        self.feature_stds = torch.Tensor(feature_stds)\\n        self.feature_means = torch.Tensor(feature_means)\\n\\n    def __len__(self):\\n        return len(self.data)\\n\\n    def __getitem__(self, idx):\\n        sample = self.data[idx]\\n        features = torch.Tensor(sample['features'].toArray())\\n        normalized_features = (features - self.feature_means) / self.feature_stds\\n        return {'features': normalized_features, 'pos': sample['pos']}\\n\\ndef collate_fn(batch):\\n    features = [item['features'] for item in batch]\\n    pos = [item['pos'] for item in batch]\\n    return {'features': torch.stack(features), 'pos': pos}\\n\\ndef calculate_feature_stds(parquet_dir):\\n    spark = SparkSession.builder.getOrCreate()\\n    df = spark.read.parquet(parquet_dir)\\n    features_rdd = df.select('features').rdd.map(lambda row: np.array(row['features'].toArray()))\\n    all_features = np.vstack(features_rdd.collect())\\n    feature_stds = all_features.std(axis=0)\\n    feature_means = all_features.mean(axis=0)\\n    return feature_stds, feature_means\\n\\ndef create_dataloader(parquet_dir, batch_size, shuffle):\\n    feature_stds, feature_means = calculate_feature_stds(parquet_dir)\\n    dataset = ParquetDataset(parquet_dir, feature_stds, feature_means)\\n    return DataLoader(dataset, batch_size=batch_size, shuffle=shuffle, collate_fn=collate_fn)\\n\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 8
        }
      ],
      "source": [
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from pyspark.sql import SparkSession\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "class ParquetDataset(Dataset):\n",
        "    def __init__(self, parquet_dir, global_std, global_mean):\n",
        "        self.spark = SparkSession.builder.getOrCreate()\n",
        "        self.df = self.spark.read.parquet(parquet_dir).sort('pos')\n",
        "        self.data = self.df.collect()\n",
        "        self.global_std = 1\n",
        "        self.global_mean = 0\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        sample = self.data[idx]\n",
        "        features = torch.Tensor(sample['features'].toArray())\n",
        "        normalized_features = (features - self.global_mean) / self.global_std\n",
        "        return {'features': normalized_features, 'pos': sample['pos']}\n",
        "\n",
        "def collate_fn(batch):\n",
        "    features = [item['features'] for item in batch]\n",
        "    pos = [item['pos'] for item in batch]\n",
        "    return {'features': torch.stack(features), 'pos': pos}\n",
        "\n",
        "\n",
        "def calculate_global_mean_std(parquet_dir):\n",
        "    spark = SparkSession.builder.getOrCreate()\n",
        "    df = spark.read.parquet(parquet_dir)\n",
        "    features_rdd = df.select('features').rdd.map(lambda row: np.array(row['features'].toArray()))\n",
        "    all_features = np.vstack(features_rdd.collect())\n",
        "    global_std = all_features.std()\n",
        "    global_mean = all_features.mean()\n",
        "    return global_std, global_mean\n",
        "\n",
        "def create_dataloader(parquet_dir, batch_size, shuffle):\n",
        "    global_std, global_mean = calculate_global_mean_std(parquet_dir)\n",
        "    dataset = ParquetDataset(parquet_dir, global_std, global_mean)\n",
        "    return DataLoader(dataset, batch_size=batch_size, shuffle=shuffle, collate_fn=collate_fn, drop_last=True)\n",
        "\n",
        "'''\n",
        "\n",
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from pyspark.sql import SparkSession\n",
        "import numpy as np\n",
        "\n",
        "class ParquetDataset(Dataset):\n",
        "    def __init__(self, parquet_dir, feature_stds, feature_means):\n",
        "        self.spark = SparkSession.builder.getOrCreate()\n",
        "        self.df = self.spark.read.parquet(parquet_dir).sort('pos')\n",
        "        self.data = self.df.collect()\n",
        "        self.feature_stds = torch.Tensor(feature_stds)\n",
        "        self.feature_means = torch.Tensor(feature_means)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        sample = self.data[idx]\n",
        "        features = torch.Tensor(sample['features'].toArray())\n",
        "        normalized_features = (features - self.feature_means) / self.feature_stds\n",
        "        return {'features': normalized_features, 'pos': sample['pos']}\n",
        "\n",
        "def collate_fn(batch):\n",
        "    features = [item['features'] for item in batch]\n",
        "    pos = [item['pos'] for item in batch]\n",
        "    return {'features': torch.stack(features), 'pos': pos}\n",
        "\n",
        "def calculate_feature_stds(parquet_dir):\n",
        "    spark = SparkSession.builder.getOrCreate()\n",
        "    df = spark.read.parquet(parquet_dir)\n",
        "    features_rdd = df.select('features').rdd.map(lambda row: np.array(row['features'].toArray()))\n",
        "    all_features = np.vstack(features_rdd.collect())\n",
        "    feature_stds = all_features.std(axis=0)\n",
        "    feature_means = all_features.mean(axis=0)\n",
        "    return feature_stds, feature_means\n",
        "\n",
        "def create_dataloader(parquet_dir, batch_size, shuffle):\n",
        "    feature_stds, feature_means = calculate_feature_stds(parquet_dir)\n",
        "    dataset = ParquetDataset(parquet_dir, feature_stds, feature_means)\n",
        "    return DataLoader(dataset, batch_size=batch_size, shuffle=shuffle, collate_fn=collate_fn)\n",
        "'''"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "6zgDJK6Z2zh7"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "\n",
        "\n",
        "def noise(vec):\n",
        "    noise = np.random.normal(1, .1, vec.shape)\n",
        "    vec = vec * torch.Tensor(noise)\n",
        "    return vec.float()\n",
        "\n",
        "\n",
        "class LambdaLayer(nn.Module):\n",
        "    def __init__(self, lambd):\n",
        "        super(LambdaLayer, self).__init__()\n",
        "        self.lambd = lambd\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.lambd(x)\n",
        "\n",
        "\n",
        "class ReshapeLayer(nn.Module):\n",
        "    def __init__(self, vec_shape):\n",
        "        super(ReshapeLayer, self).__init__()\n",
        "        self.vec_shape = vec_shape\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x.view([x.shape[0]] + self.vec_shape)\n",
        "        return x\n",
        "\n",
        "\n",
        "class SwapLastTwoDimensions(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(SwapLastTwoDimensions, self).__init__()\n",
        "\n",
        "    def forward(self, x):\n",
        "        return x.permute(0, 1, 3, 2) if x.dim() == 4 else x.permute(0, 2, 1)\n",
        "\n",
        "\n",
        "class ANN(nn.Module):\n",
        "    def __init__(self, input_dim, kernel_sizes, last_dim, hidden_dims, strides, device='cpu'):\n",
        "        super().__init__()\n",
        "        self.device = device\n",
        "        self.loss_function = nn.CrossEntropyLoss()\n",
        "        self.activation = nn.ReLU()\n",
        "        self.net = nn.ModuleList()\n",
        "        self.dims = [input_dim]  # To store the dimensions at each layer\n",
        "\n",
        "        current_dim = input_dim\n",
        "\n",
        "        # Sequence of blocks: Convolutional Layer, Normalization Layer, ReLU\n",
        "        for i in range(len(kernel_sizes)):\n",
        "            kernel_size = kernel_sizes[i]\n",
        "            hidden_dim = hidden_dims[i]\n",
        "            next_hidden_dim = hidden_dims[i+1]\n",
        "            stride = strides[i]\n",
        "            self.net.append(nn.Conv1d(hidden_dim, next_hidden_dim, kernel_size, stride=stride))\n",
        "            new_dim = (current_dim - kernel_size) // stride + 1\n",
        "            self.net.append(nn.LayerNorm(new_dim))\n",
        "            self.net.append(self.activation)\n",
        "            current_dim = new_dim\n",
        "            self.dims.append(current_dim)  # Save the new dimension\n",
        "\n",
        "        # Final output block: Linear Layer followed by Normalization Layer\n",
        "        self.net.append(nn.Flatten())\n",
        "        self.net.append(nn.Linear(current_dim * hidden_dims[-1], last_dim))\n",
        "        self.net.append(nn.LayerNorm(last_dim, ))\n",
        "\n",
        "        self.dims.append(last_dim)  # Save the last dimension\n",
        "\n",
        "        self.decoder = get_decoder(self.dims, kernel_sizes, hidden_dims, strides, device=device)\n",
        "\n",
        "        self.net.to(device)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x.unsqueeze(1)\n",
        "        for module in self.net:\n",
        "            x = module(x)\n",
        "\n",
        "        x = F.normalize(x, p=2, dim=1)\n",
        "        emb = x\n",
        "\n",
        "\n",
        "        for module in self.decoder:\n",
        "            x = module(x)\n",
        "        x = x.squeeze(1)\n",
        "\n",
        "        return emb, x\n",
        "\n",
        "\n",
        "def get_decoder(dims, kernel_sizes, hidden_dims, strides, device='cpu'):\n",
        "      activation = nn.ReLU()\n",
        "      net = nn.ModuleList()\n",
        "\n",
        "      # Initial block: Linear layer, Normalization, Transposed Convolution\n",
        "\n",
        "      net.append(nn.Linear(dims[-1], dims[-2] * hidden_dims[-1]))\n",
        "      net.append(nn.LayerNorm(dims[-2] * hidden_dims[-1]))\n",
        "      net.append(ReshapeLayer([hidden_dims[-1], dims[-2]]))\n",
        "\n",
        "      kernel_size = kernel_sizes[-1]\n",
        "      stride = strides[-1]\n",
        "      net.append(nn.ConvTranspose1d(hidden_dims[-1], hidden_dims[-2], kernel_size, stride=stride))\n",
        "\n",
        "      # Series of blocks: Normalization, ReLU activation, Transposed Convolution\n",
        "      for i in range(len(kernel_sizes) - 1, 0, -1):\n",
        "          current_dim = dims[i]\n",
        "          net.append(nn.LayerNorm(current_dim))\n",
        "          net.append(activation)\n",
        "          kernel_size = kernel_sizes[i-1]\n",
        "          stride = strides[i-1]\n",
        "          net.append(nn.ConvTranspose1d(hidden_dims[i], hidden_dims[i-1], kernel_size, stride=stride))\n",
        "\n",
        "      net.to(device)\n",
        "      return net\n",
        "\n",
        "class EncoderDecoder(nn.Module):\n",
        "    def __init__(self, input_dim, kernel_sizes, last_dim, hidden_dims, strides, device):\n",
        "        super().__init__()\n",
        "        self.embedding = ANN(input_dim, kernel_sizes, last_dim, hidden_dims, strides, device)\n",
        "        self.device = device\n",
        "\n",
        "    def forward(self, X):\n",
        "        embedding, reconstruction = self.embedding(X)\n",
        "        return embedding, reconstruction\n",
        "\n",
        "\n",
        "def compute_mse_similarity_matrix(representations):\n",
        "    \"\"\"\n",
        "    Computes the similarity matrix based on Mean Squared Error (MSE).\n",
        "\n",
        "    Parameters:\n",
        "    representations (torch.Tensor): A tensor of shape (N, D) where N is the number of representations\n",
        "                                    and D is the dimensionality of each representation.\n",
        "\n",
        "    Returns:\n",
        "    torch.Tensor: A tensor of shape (N, N) containing the MSE-based similarity matrix.\n",
        "    \"\"\"\n",
        "    # Compute the pairwise differences\n",
        "    differences = representations.unsqueeze(1) - representations.unsqueeze(0)\n",
        "\n",
        "    # Compute the squared differences\n",
        "    squared_differences = differences ** 2\n",
        "\n",
        "    # Compute the mean of the squared differences (MSE)\n",
        "    mse_matrix = squared_differences.mean(dim=2)\n",
        "\n",
        "    return mse_matrix\n",
        "\n",
        "\n",
        "class ContrastiveLoss(nn.Module):\n",
        "    def __init__(self, batch_size, device, temperature=2):\n",
        "        super().__init__()\n",
        "        self.batch_size = batch_size\n",
        "        self.temperature = temperature\n",
        "        self.mask = (~torch.eye(batch_size * 2, batch_size * 2, dtype=bool)).float().to(device)\n",
        "\n",
        "    def forward(self, emb_i, emb_j, encoder_inputs, decoder_outputs, epoch_losses):\n",
        "        representations = torch.cat([emb_i, emb_j], dim=0)\n",
        "        similarity_matrix = F.cosine_similarity(representations.unsqueeze(1), representations.unsqueeze(0), dim=2)\n",
        "\n",
        "        sim_ij = torch.diag(similarity_matrix, self.batch_size)\n",
        "        sim_ji = torch.diag(similarity_matrix, -self.batch_size)\n",
        "\n",
        "        positives = torch.cat([sim_ij, sim_ji], dim=0)\n",
        "\n",
        "        nominator = torch.exp(positives / self.temperature)\n",
        "\n",
        "        denominator = self.mask * torch.exp(similarity_matrix / self.temperature)\n",
        "\n",
        "        all_losses = -torch.log(nominator / torch.sum(denominator, dim=1))\n",
        "        contrastive_loss = torch.sum(all_losses) / (2 * self.batch_size)\n",
        "\n",
        "        #similarity_matrix = compute_mse_similarity_matrix(representations)\n",
        "\n",
        "        # Mean & Std Loss\n",
        "        std_loss = torch.mean((torch.std(representations, dim=0) - 1) ** 2)\n",
        "        mean_loss = torch.mean(torch.mean(representations, dim=1) ** 2)\n",
        "\n",
        "        # MSE Loss\n",
        "        decoder_loss = torch.mean(torch.sqrt(torch.mean((encoder_inputs - decoder_outputs) ** 2, dim=1)))\n",
        "\n",
        "        # Combine the losses\n",
        "        total_loss = contrastive_loss * 1e-2 + std_loss * 1e-3 + mean_loss * 1e-3 + decoder_loss\n",
        "\n",
        "        epoch_losses['contrastive_loss'] += contrastive_loss.item()\n",
        "        epoch_losses['std_loss'] += std_loss.item()\n",
        "        epoch_losses['mean_loss'] += mean_loss.item()\n",
        "        epoch_losses['decoder_loss'] += decoder_loss.item()\n",
        "        epoch_losses['total_loss'] += total_loss.item()\n",
        "\n",
        "        return total_loss\n",
        "\n",
        "\n",
        "def get_clr_training_components(hparams_CLR):\n",
        "    model_CLR = EncoderDecoder(hparams_CLR.input_dim, hparams_CLR.kernel_sizes, hparams_CLR.last_dim, hparams_CLR.hidden_dims, hparams_CLR.strides, hparams_CLR.device)\n",
        "    criterion_CLR = ContrastiveLoss(hparams_CLR.batch_size, hparams_CLR.device)\n",
        "    optimizer_CLR = Adam(model_CLR.parameters(), lr=hparams_CLR.lr, weight_decay=1e-5)\n",
        "    scheduler_CLR = CosineAnnealingLR(optimizer_CLR, hparams_CLR.epochs // 10)\n",
        "    return model_CLR, criterion_CLR, optimizer_CLR, scheduler_CLR\n",
        "\n",
        "\n",
        "def train_clr(model, criterion, optimizer, dataloader, batch_step, epoch_losses_list):\n",
        "    losses = []\n",
        "    i = 0\n",
        "    model.train()\n",
        "    X = next(dataloader, None)\n",
        "\n",
        "    epoch_losses = {\n",
        "        'contrastive_loss': 0,\n",
        "        'std_loss': 0,\n",
        "        'mean_loss': 0,\n",
        "        'decoder_loss': 0,\n",
        "        'total_loss': 0\n",
        "    }\n",
        "\n",
        "    while i < batch_step and X is not None:\n",
        "        optimizer.zero_grad()\n",
        "        input_1 = torch.Tensor(X['features']).to(model.device)\n",
        "        input_2 = torch.Tensor(noise(X['features'])).to(model.device)\n",
        "        emb_1, decoded_1 = model(input_1)\n",
        "        emb_2, decoded_2 = model(input_2)\n",
        "        loss = criterion(emb_1, emb_2, torch.cat((input_1, input_2)), torch.cat((decoded_1, decoded_2)), epoch_losses)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        losses.append(loss.detach().cpu().numpy())\n",
        "        i += 1\n",
        "        X = next(dataloader, None)\n",
        "\n",
        "    epoch_losses['contrastive_loss'] /= i\n",
        "    epoch_losses['std_loss'] /= i\n",
        "    epoch_losses['mean_loss'] /= i\n",
        "    epoch_losses['decoder_loss'] /= i\n",
        "\n",
        "    epoch_losses_list.append(epoch_losses)\n",
        "    return sum(losses)\n",
        "\n",
        "\n",
        "def fit_clr(hparams_CLR, output_folder, dataloader):\n",
        "    epoch_losses_list = []\n",
        "    if not os.path.exists(output_folder):\n",
        "      os.makedirs(output_folder)\n",
        "    model_CLR, criterion_CLR, optimizer_CLR, scheduler_CLR = get_clr_training_components(hparams_CLR)\n",
        "    total_losses = []\n",
        "    best_loss = np.inf\n",
        "    patience = 0\n",
        "    for epoch in range(hparams_CLR.epochs):\n",
        "        if patience > hparams_CLR.patience:\n",
        "            break\n",
        "        loss = train_clr(model_CLR, criterion_CLR, optimizer_CLR, iter(dataloader), hparams_CLR.batch_step, epoch_losses_list)\n",
        "        scheduler_CLR.step()\n",
        "        total_losses.append(loss)\n",
        "        if best_loss > loss:\n",
        "            torch.save(model_CLR.state_dict(), output_folder + '/model_CLR.ckpt')\n",
        "            np.save(output_folder + '/losses_CLR.npy', total_losses)\n",
        "            best_loss = loss\n",
        "            patience = 0\n",
        "        else:\n",
        "            patience += 1\n",
        "        print(f'Epoch {epoch}: loss: {loss}')\n",
        "\n",
        "    with open(output_folder + '/epoch_losses.json', 'w') as f:\n",
        "        json.dump(epoch_losses_list, f)\n",
        "\n",
        "    return model_CLR, total_losses, epoch_losses_list\n",
        "\n",
        "\n",
        "def run_thru_NN(model, dataloader):\n",
        "    output = []\n",
        "    model.eval()\n",
        "    X = next(dataloader, None)\n",
        "    while X is not None:\n",
        "        X = torch.Tensor(X['features']).to(model.device)\n",
        "        with torch.no_grad():\n",
        "            output_batch, _ = model(X)\n",
        "        output.append(output_batch.detach().cpu().numpy())\n",
        "        X = next(dataloader, None)\n",
        "    output = np.concatenate(output, axis=0)\n",
        "    return output\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Nk-OKL2I3Ggj",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9df93a45-2d25-47fa-d79c-9539164bf035"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 0: loss: 750.5925188064575\n",
            "Epoch 1: loss: 679.4657425880432\n",
            "Epoch 2: loss: 598.2769484519958\n",
            "Epoch 3: loss: 518.2337839603424\n",
            "Epoch 4: loss: 464.8858780860901\n",
            "Epoch 5: loss: 423.4069867134094\n",
            "Epoch 6: loss: 390.4490444660187\n",
            "Epoch 7: loss: 363.6869218349457\n",
            "Epoch 8: loss: 343.5916533470154\n",
            "Epoch 9: loss: 327.5324239730835\n",
            "Epoch 10: loss: 313.32241702079773\n"
          ]
        }
      ],
      "source": [
        "# Example usage\n",
        "dataloader = create_dataloader(parquet_dir, hparams_CLR.batch_size, True)\n",
        "\n",
        "# Train the model\n",
        "model_CLR, total_losses, epoch_losses_list = fit_clr(hparams_CLR, output_folder, dataloader)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zDUqKsWjVXx6"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "\n",
        "def plot_epoch_losses(epoch_losses_list, title=\"Training Losses\", xlabel=\"Epochs\", ylabel=\"Loss\"):\n",
        "    \"\"\"\n",
        "    Plots the losses for each epoch from a list of dictionaries.\n",
        "\n",
        "    Parameters:\n",
        "    - epoch_losses_list (list of dict): The list containing dictionaries of loss values for each epoch.\n",
        "    - title (str): The title of the plot.\n",
        "    - xlabel (str): The label for the x-axis.\n",
        "    - ylabel (str): The label for the y-axis.\n",
        "    \"\"\"\n",
        "\n",
        "    # Initialize dictionaries to hold losses by type\n",
        "    losses_by_type = {\n",
        "        'contrastive_loss': [],\n",
        "        'std_loss': [],\n",
        "        'mean_loss': [],\n",
        "        'decoder_loss': [],\n",
        "        'total_loss': []\n",
        "    }\n",
        "\n",
        "    # Extract losses for each epoch\n",
        "    for epoch_losses in epoch_losses_list:\n",
        "        for key in losses_by_type.keys():\n",
        "            losses_by_type[key].append(epoch_losses[key])\n",
        "\n",
        "    plt.figure(figsize=(20, 5))  # Adjust the width to accommodate more subplots\n",
        "\n",
        "    # Plot each type of loss in its own subplot\n",
        "    for idx, (key, losses) in enumerate(losses_by_type.items(), 1):\n",
        "        plt.subplot(1, 5, idx)  # 1 row, 5 columns, current plot index\n",
        "        plt.plot(losses, label=key)\n",
        "        plt.title(key)\n",
        "        plt.xlabel(xlabel)\n",
        "        plt.ylabel(ylabel)\n",
        "        plt.grid(True)\n",
        "\n",
        "    plt.suptitle(title)  # Super title for all subplots\n",
        "    plt.tight_layout(rect=[0, 0, 1, 0.95])  # Adjust layout to fit title\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "plot_epoch_losses(epoch_losses_list)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CzqIQA5pUbMc"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def run_thru_NN(model, parquet_dir):\n",
        "    dataloader = iter(create_dataloader(parquet_dir, batch_size=1, shuffle=False))\n",
        "    model.eval()\n",
        "\n",
        "    collected_output_vectors = []\n",
        "\n",
        "    au = 0\n",
        "\n",
        "    X = next(dataloader, None)\n",
        "    while X is not None:\n",
        "        X_tensor = torch.Tensor(X['features']).to(model.device)\n",
        "        with torch.no_grad():\n",
        "            output_batch, decoder_output_batch = model(X_tensor)\n",
        "\n",
        "        input_vector = X_tensor.detach().cpu().numpy().squeeze()\n",
        "        output_vector = output_batch.detach().cpu().numpy().squeeze()\n",
        "        decoder_output = decoder_output_batch.detach().cpu().numpy().squeeze()\n",
        "\n",
        "        collected_output_vectors.append(output_vector)\n",
        "        au += 1\n",
        "        if au in [1, 2, 715, 716, 925, 926, 1599, 1600]:\n",
        "            print(X['pos'][0])\n",
        "            plot_timeseries(input_vector, output_vector, decoder_output, noise(X_tensor).detach().cpu().numpy().squeeze())\n",
        "\n",
        "        X = next(dataloader, None)\n",
        "\n",
        "\n",
        "    # Convert collected output vectors to a numpy array and save\n",
        "    collected_output_vectors = np.array(collected_output_vectors)\n",
        "    output_file_path = os.path.join(output_folder, 'CLR_feat.npy')\n",
        "    np.save(output_file_path, collected_output_vectors)\n",
        "    print(f\"Output vectors saved to {output_file_path}\")\n",
        "\n",
        "\n",
        "def plot_timeseries(input_vector, output_vector, decoder_output, noised):\n",
        "    fig, axes = plt.subplots(1, 4, figsize=(18, 6))\n",
        "\n",
        "    # Plot input vector as time series\n",
        "    axes[0].plot(input_vector, 'r-', alpha=0.5)\n",
        "    axes[0].set_title('Input Vector Time Series')\n",
        "    axes[0].legend(['Input Vector'])\n",
        "\n",
        "    # Plot output vector as time series\n",
        "    axes[1].plot(output_vector, 'g-', alpha=0.5)\n",
        "    axes[1].set_title('Output Vector Time Series')\n",
        "    axes[1].legend(['Output Vector'])\n",
        "\n",
        "    # Plot decoder output vector as time series\n",
        "    axes[2].plot(decoder_output, 'b-', alpha=0.5)\n",
        "    axes[2].set_title('Decoder Output Vector Time Series')\n",
        "    axes[2].legend(['Decoder Output'])\n",
        "\n",
        "    # Plot noised input vector as time series\n",
        "    axes[3].plot(noised, 'b-', alpha=0.5)\n",
        "    axes[3].set_title('Noised Vector Time Series')\n",
        "    axes[3].legend(['Noised Output'])\n",
        "\n",
        "    plt.show()\n",
        "\n",
        "run_thru_NN(model_CLR, parquet_dir)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SmHWA5NyPPdI"
      },
      "outputs": [],
      "source": [
        "# Your code block here\n",
        "\n",
        "end_time = time.time()\n",
        "execution_time = end_time - start_time\n",
        "print(f\"Execution time: {execution_time} seconds\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rznBjpfqLpKa"
      },
      "outputs": [],
      "source": [
        "for key, value in model_CLR.state_dict().items():\n",
        "    print(f\"{key}: {value.shape}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uza8x3Pyd9Dd"
      },
      "outputs": [],
      "source": [
        "if False:\n",
        "    checkpoint_path = \"/content/drive/My Drive/MSI-Segmentation/results/sztuczne_dane_08-11-2023-10-51_conv_True/model_CLR.ckpt\"\n",
        "    checkpoint = torch.load(checkpoint_path, map_location=torch.device('cpu'))\n",
        "    model_CLR.load_state_dict(checkpoint)  # Load the state dict\n",
        "\n",
        "    run_thru_NN(model_CLR, parquet_dir)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QMXga1_rjJSG"
      },
      "outputs": [],
      "source": [
        "if False:\n",
        "    checkpoint_path = \"/content/drive/My Drive/MSI-Segmentation/results/sztuczne_dane_25-07-2024-20-25_conv_True/model_CLR.ckpt\"\n",
        "    checkpoint = torch.load(checkpoint_path, map_location=torch.device('cpu'))\n",
        "    model_CLR.load_state_dict(checkpoint)  # Load the state dict\n",
        "\n",
        "    run_thru_NN(model_CLR, parquet_dir)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "L4",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}