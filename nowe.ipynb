{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kskrajny/MSI-Segmentation/blob/master/nowe.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "m_DiStjf09TH",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 140
        },
        "outputId": "391f0488-e1bc-4871-de91-642a9e9de692"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (2.0.3)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas) (2024.1)\n",
            "Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas) (2024.1)\n",
            "Requirement already satisfied: numpy>=1.21.0 in /usr/local/lib/python3.10/dist-packages (from pandas) (1.25.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\n# Set up Spark session\\nspark = SparkSession.builder     .master(\"local\")     .appName(\"Magisterka\")     .config(\"spark.driver.memory\", \"9g\")     .config(\"spark.driver.maxResultSize\", \"0\")     .getOrCreate()\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 4
        }
      ],
      "source": [
        "# Install dependencies\n",
        "#!pip install pyspark\n",
        "!pip install pandas\n",
        "\n",
        "# Import libraries\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "from torch import nn\n",
        "from torch.optim import Adam\n",
        "import torch.nn.functional as F\n",
        "from torch.optim.lr_scheduler import CosineAnnealingLR\n",
        "import os\n",
        "import time\n",
        "\n",
        "start_time = time.time()\n",
        "'''\n",
        "# Set up Spark session\n",
        "spark = SparkSession.builder \\\n",
        "    .master(\"local\") \\\n",
        "    .appName(\"Magisterka\") \\\n",
        "    .config(\"spark.driver.memory\", \"9g\") \\\n",
        "    .config(\"spark.driver.maxResultSize\", \"0\") \\\n",
        "    .getOrCreate()\n",
        "'''"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "ORfClzfrKhOT"
      },
      "outputs": [],
      "source": [
        "from datetime import datetime\n",
        "import pytz\n",
        "\n",
        "poland_tz = pytz.timezone('Europe/Warsaw')\n",
        "current_time_poland = datetime.now(poland_tz)\n",
        "formatted_date = current_time_poland.strftime('%d-%m-%Y-%H-%M')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "Wcp1pemj3WeX",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "601a89fb-343e-4b31-d8d2-109fcb4465a1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "dataset = \"nowe_dane\"\n",
        "\n",
        "if dataset == \"nowe_dane\":\n",
        "  input_dim = 44000\n",
        "  last_dim = 128\n",
        "  kernel_sizes = [15, 11, 5, 3, 5]\n",
        "  strides = [5, 3, 3, 7, 3]\n",
        "  hidden_dims = [1, 4, 8, 13, 20, 35]\n",
        "\n",
        "\n",
        "convolve = \"False\"\n",
        "\n",
        "parquet_dir = \"/content/drive/My Drive/MSI-Segmentation/dane/\" + dataset + \"/parquet_convolve_\" + convolve\n",
        "output_folder = \"/content/drive/My Drive/MSI-Segmentation/results/\" + dataset + f\"_{formatted_date}_conv_\" + convolve\n",
        "\n",
        "class HParams:\n",
        "    def __init__(self, input_dim, kernel_sizes, last_dim, hidden_dims, strides, batch_size=64, lr=1e-3, epochs=200, batch_step=1600, patience=30):\n",
        "        self.input_dim = input_dim\n",
        "        self.kernel_sizes = kernel_sizes\n",
        "        self.last_dim = last_dim\n",
        "        self.hidden_dims = hidden_dims\n",
        "        self.device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "        self.batch_size = batch_size\n",
        "        self.lr = lr\n",
        "        self.epochs = epochs\n",
        "        self.batch_step = batch_step\n",
        "        self.patience = patience\n",
        "        self.strides = strides\n",
        "\n",
        "hparams_CLR = HParams(input_dim, kernel_sizes, last_dim, hidden_dims, strides)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "6zgDJK6Z2zh7"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "\n",
        "\n",
        "def noise(vec):\n",
        "    noise = np.random.normal(1, .1, vec.shape)\n",
        "    vec = vec * torch.Tensor(noise)\n",
        "    return vec.float()\n",
        "\n",
        "\n",
        "class LambdaLayer(nn.Module):\n",
        "    def __init__(self, lambd):\n",
        "        super(LambdaLayer, self).__init__()\n",
        "        self.lambd = lambd\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.lambd(x)\n",
        "\n",
        "\n",
        "class ReshapeLayer(nn.Module):\n",
        "    def __init__(self, vec_shape):\n",
        "        super(ReshapeLayer, self).__init__()\n",
        "        self.vec_shape = vec_shape\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x.view([x.shape[0]] + self.vec_shape)\n",
        "        return x\n",
        "\n",
        "\n",
        "class SwapLastTwoDimensions(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(SwapLastTwoDimensions, self).__init__()\n",
        "\n",
        "    def forward(self, x):\n",
        "        return x.permute(0, 1, 3, 2) if x.dim() == 4 else x.permute(0, 2, 1)\n",
        "\n",
        "\n",
        "class ANN(nn.Module):\n",
        "    def __init__(self, input_dim, kernel_sizes, last_dim, hidden_dims, strides, device='cpu'):\n",
        "        super().__init__()\n",
        "        self.device = device\n",
        "        self.loss_function = nn.CrossEntropyLoss()\n",
        "        self.activation = nn.ReLU()\n",
        "        self.net = nn.ModuleList()\n",
        "        self.dims = [input_dim]  # To store the dimensions at each layer\n",
        "\n",
        "        current_dim = input_dim\n",
        "\n",
        "        # Sequence of blocks: Convolutional Layer, Normalization Layer, ReLU\n",
        "        for i in range(len(kernel_sizes)):\n",
        "            kernel_size = kernel_sizes[i]\n",
        "            hidden_dim = hidden_dims[i]\n",
        "            next_hidden_dim = hidden_dims[i+1]\n",
        "            stride = strides[i]\n",
        "            self.net.append(nn.Conv1d(hidden_dim, next_hidden_dim, kernel_size, stride=stride))\n",
        "            new_dim = (current_dim - kernel_size) // stride + 1\n",
        "            self.net.append(nn.LayerNorm(new_dim))\n",
        "            self.net.append(self.activation)\n",
        "            current_dim = new_dim\n",
        "            self.dims.append(current_dim)  # Save the new dimension\n",
        "\n",
        "        # Final output block: Linear Layer followed by Normalization Layer\n",
        "        self.net.append(nn.Flatten())\n",
        "        self.net.append(nn.Linear(current_dim * hidden_dims[-1], last_dim))\n",
        "        self.net.append(nn.LayerNorm(last_dim, ))\n",
        "\n",
        "        self.dims.append(last_dim)  # Save the last dimension\n",
        "\n",
        "        self.decoder = get_decoder(self.dims, kernel_sizes, hidden_dims, strides, device=device)\n",
        "\n",
        "        self.net.to(device)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x.unsqueeze(1)\n",
        "        for module in self.net:\n",
        "            x = module(x)\n",
        "        x = F.normalize(x, p=2, dim=1)\n",
        "        emb = x\n",
        "\n",
        "\n",
        "        for module in self.decoder:\n",
        "            x = module(x)\n",
        "        x = x.squeeze(1)\n",
        "\n",
        "        return emb, x\n",
        "\n",
        "\n",
        "def get_decoder(dims, kernel_sizes, hidden_dims, strides, device='cpu'):\n",
        "      activation = nn.ReLU()\n",
        "      net = nn.ModuleList()\n",
        "\n",
        "      # Initial block: Linear layer, Normalization, Transposed Convolution\n",
        "\n",
        "      net.append(nn.Linear(dims[-1], dims[-2] * hidden_dims[-1]))\n",
        "      net.append(nn.LayerNorm(dims[-2] * hidden_dims[-1]))\n",
        "      net.append(ReshapeLayer([hidden_dims[-1], dims[-2]]))\n",
        "\n",
        "      kernel_size = kernel_sizes[-1]\n",
        "      stride = strides[-1]\n",
        "      net.append(nn.ConvTranspose1d(hidden_dims[-1], hidden_dims[-2], kernel_size, stride=stride))\n",
        "\n",
        "      # Series of blocks: Normalization, ReLU activation, Transposed Convolution\n",
        "      for i in range(len(kernel_sizes) - 1, 0, -1):\n",
        "          current_dim = dims[i]\n",
        "          net.append(nn.LayerNorm(current_dim))\n",
        "          net.append(activation)\n",
        "          kernel_size = kernel_sizes[i-1]\n",
        "          stride = strides[i-1]\n",
        "          net.append(nn.ConvTranspose1d(hidden_dims[i], hidden_dims[i-1], kernel_size, stride=stride))\n",
        "\n",
        "      net.to(device)\n",
        "      return net\n",
        "\n",
        "class EncoderDecoder(nn.Module):\n",
        "    def __init__(self, input_dim, kernel_sizes, last_dim, hidden_dims, strides, device):\n",
        "        super().__init__()\n",
        "        self.embedding = ANN(input_dim, kernel_sizes, last_dim, hidden_dims, strides, device)\n",
        "        self.device = device\n",
        "\n",
        "    def forward(self, X):\n",
        "        embedding, reconstruction = self.embedding(X)\n",
        "        return embedding, reconstruction\n",
        "\n",
        "\n",
        "def compute_mse_similarity_matrix(representations):\n",
        "    \"\"\"\n",
        "    Computes the similarity matrix based on Mean Squared Error (MSE).\n",
        "\n",
        "    Parameters:\n",
        "    representations (torch.Tensor): A tensor of shape (N, D) where N is the number of representations\n",
        "                                    and D is the dimensionality of each representation.\n",
        "\n",
        "    Returns:\n",
        "    torch.Tensor: A tensor of shape (N, N) containing the MSE-based similarity matrix.\n",
        "    \"\"\"\n",
        "    # Compute the pairwise differences\n",
        "    differences = representations.unsqueeze(1) - representations.unsqueeze(0)\n",
        "\n",
        "    # Compute the squared differences\n",
        "    squared_differences = differences ** 2\n",
        "\n",
        "    # Compute the mean of the squared differences (MSE)\n",
        "    mse_matrix = squared_differences.mean(dim=2)\n",
        "\n",
        "    return mse_matrix\n",
        "\n",
        "\n",
        "class ContrastiveLoss(nn.Module):\n",
        "    def __init__(self, batch_size, device, temperature=2):\n",
        "        super().__init__()\n",
        "        self.batch_size = batch_size\n",
        "        self.temperature = temperature\n",
        "        self.mask = (~torch.eye(batch_size * 2, batch_size * 2, dtype=bool)).float().to(device)\n",
        "\n",
        "    def forward(self, emb_i, emb_j, encoder_inputs, decoder_outputs, epoch_losses):\n",
        "        representations = torch.cat([emb_i, emb_j], dim=0)\n",
        "        similarity_matrix = F.cosine_similarity(representations.unsqueeze(1), representations.unsqueeze(0), dim=2)\n",
        "\n",
        "        sim_ij = torch.diag(similarity_matrix, self.batch_size)\n",
        "        sim_ji = torch.diag(similarity_matrix, -self.batch_size)\n",
        "\n",
        "        positives = torch.cat([sim_ij, sim_ji], dim=0)\n",
        "\n",
        "        nominator = torch.exp(positives / self.temperature)\n",
        "        denominator = self.mask * torch.exp(similarity_matrix / self.temperature)\n",
        "\n",
        "        all_losses = -torch.log(nominator / torch.sum(denominator, dim=1))\n",
        "        contrastive_loss = torch.sum(all_losses) / (2 * self.batch_size)\n",
        "\n",
        "        #similarity_matrix = compute_mse_similarity_matrix(representations)\n",
        "\n",
        "        # Mean & Std Loss\n",
        "        std_loss = torch.mean((torch.std(representations, dim=0) - 1) ** 2)\n",
        "        mean_loss = torch.mean(torch.mean(representations, dim=1) ** 2)\n",
        "\n",
        "        # MSE Loss\n",
        "        decoder_loss = torch.mean(torch.sqrt(torch.mean((encoder_inputs - decoder_outputs) ** 2, dim=1)))\n",
        "\n",
        "        # Combine the losses\n",
        "        total_loss = contrastive_loss * 1e-2 + std_loss * 1e-3 + mean_loss * 1e-3 + decoder_loss\n",
        "\n",
        "        epoch_losses['contrastive_loss'] += contrastive_loss.item()\n",
        "        epoch_losses['std_loss'] += std_loss.item()\n",
        "        epoch_losses['mean_loss'] += mean_loss.item()\n",
        "        epoch_losses['decoder_loss'] += decoder_loss.item()\n",
        "        epoch_losses['total_loss'] += total_loss.item()\n",
        "\n",
        "        return total_loss\n",
        "\n",
        "\n",
        "def get_clr_training_components(hparams_CLR):\n",
        "    model_CLR = EncoderDecoder(hparams_CLR.input_dim, hparams_CLR.kernel_sizes, hparams_CLR.last_dim, hparams_CLR.hidden_dims, hparams_CLR.strides, hparams_CLR.device)\n",
        "    criterion_CLR = ContrastiveLoss(hparams_CLR.batch_size, hparams_CLR.device)\n",
        "    optimizer_CLR = Adam(model_CLR.parameters(), lr=hparams_CLR.lr, weight_decay=1e-5)\n",
        "    scheduler_CLR = CosineAnnealingLR(optimizer_CLR, hparams_CLR.epochs // 10)\n",
        "    return model_CLR, criterion_CLR, optimizer_CLR, scheduler_CLR\n",
        "\n",
        "\n",
        "def train_clr(model, criterion, optimizer, dataloader, batch_step, epoch_losses_list):\n",
        "    start = time.time()\n",
        "    losses = []\n",
        "    i = 0\n",
        "    model.train()\n",
        "    X = next(dataloader, None)\n",
        "    epoch_losses = {\n",
        "        'contrastive_loss': 0,\n",
        "        'std_loss': 0,\n",
        "        'mean_loss': 0,\n",
        "        'decoder_loss': 0,\n",
        "        'total_loss': 0\n",
        "    }\n",
        "    while i < batch_step and X is not None:\n",
        "        optimizer.zero_grad()\n",
        "        input_1 = torch.Tensor(X['features']).to(model.device)\n",
        "        input_2 = torch.Tensor(noise(X['features'])).to(model.device)\n",
        "        emb_1, decoded_1 = model(input_1)\n",
        "        emb_2, decoded_2 = model(input_2)\n",
        "        loss = criterion(emb_1, emb_2, torch.cat((input_1, input_2)), torch.cat((decoded_1, decoded_2)), epoch_losses)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        losses.append(loss.detach().cpu().numpy())\n",
        "        i += 1\n",
        "        X = next(dataloader, None)\n",
        "\n",
        "    epoch_losses['contrastive_loss'] /= i\n",
        "    epoch_losses['std_loss'] /= i\n",
        "    epoch_losses['mean_loss'] /= i\n",
        "    epoch_losses['decoder_loss'] /= i\n",
        "\n",
        "    epoch_losses_list.append(epoch_losses)\n",
        "    return sum(losses)\n",
        "\n",
        "\n",
        "def fit_clr(hparams_CLR, output_folder, dataloader):\n",
        "    epoch_losses_list = []\n",
        "    if not os.path.exists(output_folder):\n",
        "      os.makedirs(output_folder)\n",
        "    model_CLR, criterion_CLR, optimizer_CLR, scheduler_CLR = get_clr_training_components(hparams_CLR)\n",
        "    total_losses = []\n",
        "    best_loss = np.inf\n",
        "    patience = 0\n",
        "    for epoch in range(hparams_CLR.epochs):\n",
        "        if patience > hparams_CLR.patience:\n",
        "            break\n",
        "        loss = train_clr(model_CLR, criterion_CLR, optimizer_CLR, iter(dataloader), hparams_CLR.batch_step, epoch_losses_list)\n",
        "        scheduler_CLR.step()\n",
        "        total_losses.append(loss)\n",
        "        if best_loss > loss:\n",
        "            torch.save(model_CLR.state_dict(), output_folder + '/model_CLR.ckpt')\n",
        "            np.save(output_folder + '/losses_CLR.npy', total_losses)\n",
        "            best_loss = loss\n",
        "            patience = 0\n",
        "        else:\n",
        "            patience += 1\n",
        "        print(f'Epoch {epoch}: loss: {loss}')\n",
        "\n",
        "    with open(output_folder + '/epoch_losses.json', 'w') as f:\n",
        "        json.dump(epoch_losses_list, f)\n",
        "\n",
        "    return model_CLR, total_losses, epoch_losses_list\n",
        "\n",
        "\n",
        "def run_thru_NN(model, dataloader):\n",
        "    output = []\n",
        "    model.eval()\n",
        "    X = next(dataloader, None)\n",
        "    while X is not None:\n",
        "        X = torch.Tensor(X['features']).to(model.device)\n",
        "        with torch.no_grad():\n",
        "            output_batch, _ = model(X)\n",
        "        output.append(output_batch.detach().cpu().numpy())\n",
        "        X = next(dataloader, None)\n",
        "    output = np.concatenate(output, axis=0)\n",
        "    return output\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "DEneQWLh3EZ5"
      },
      "outputs": [],
      "source": [
        "'''\n",
        "from pyspark.sql import SparkSession\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import torch\n",
        "import numpy as np\n",
        "from pyspark.sql.functions import max as spark_max\n",
        "import random\n",
        "\n",
        "\n",
        "class ParquetDataset(Dataset):\n",
        "    def __init__(self, parquet_dir, global_std, global_mean, batch_size):\n",
        "        self.spark = SparkSession.builder.getOrCreate()\n",
        "        self.df = self.spark.read.parquet(parquet_dir).sort('pos').repartition('pos').cache()\n",
        "        self.global_std = global_std\n",
        "        self.global_mean = global_mean\n",
        "        self.size = self.df.count()\n",
        "        self.batch_size = batch_size\n",
        "        self.indices = list(range(self.size))\n",
        "        self.shuffle_indices()\n",
        "\n",
        "    def shuffle_indices(self):\n",
        "        random.shuffle(self.indices)\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.size\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        start_idx = idx * self.batch_size\n",
        "        end_idx = min(start_idx + self.batch_size, self.size)\n",
        "        batch_indices = self.indices[start_idx:end_idx]\n",
        "        rows = self.df.filter(self.df.pos.isin(batch_indices)).collect()\n",
        "        rows.sort(key=lambda x: batch_indices.index(x['pos']))  # Ensure the order matches batch_indices\n",
        "\n",
        "        features = [torch.Tensor(row['features'].toArray()) for row in rows]\n",
        "        normalized_features = [(feature - self.global_mean) / self.global_std for feature in features]\n",
        "        positions = [row['pos'] for row in rows]\n",
        "        return {'features': normalized_features, 'pos': positions}\n",
        "\n",
        "def collate_fn(batch):\n",
        "    features = [item for sublist in batch for item in sublist['features']]\n",
        "    pos = [item for sublist in batch for item in sublist['pos']]\n",
        "    return {'features': torch.stack(features), 'pos': pos}\n",
        "\n",
        "def calculate_global_mean_std(parquet_dir):\n",
        "    spark = SparkSession.builder.getOrCreate()\n",
        "    df = spark.read.parquet(parquet_dir)\n",
        "    features_rdd = df.select('features').rdd.map(lambda row: np.array(row['features'].toArray()))\n",
        "    all_features = np.vstack(features_rdd.collect())\n",
        "    global_std = all_features.std()\n",
        "    global_mean = all_features.mean()\n",
        "    return global_std, global_mean\n",
        "\n",
        "def create_dataloader(parquet_dir, batch_size, shuffle):\n",
        "    global_std, global_mean = calculate_global_mean_std(parquet_dir)\n",
        "    dataset = ParquetDataset(parquet_dir, global_std, global_mean)\n",
        "    return DataLoader(dataset, batch_size=batch_size, shuffle=shuffle, collate_fn=collate_fn)\n",
        "\n",
        "\n",
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from pyspark.sql import SparkSession\n",
        "import numpy as np\n",
        "\n",
        "class ParquetDataset(Dataset):\n",
        "    def __init__(self, parquet_dir, feature_stds, feature_means):\n",
        "        self.spark = SparkSession.builder.getOrCreate()\n",
        "        self.df = self.spark.read.parquet(parquet_dir).sort('pos')\n",
        "        self.data = self.df.collect()\n",
        "        self.feature_stds = torch.Tensor(feature_stds)\n",
        "        self.feature_means = torch.Tensor(feature_means)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        sample = self.data[idx]\n",
        "        features = torch.Tensor(sample['features'].toArray())\n",
        "        normalized_features = (features - self.feature_means) / self.feature_stds\n",
        "        return {'features': normalized_features, 'pos': sample['pos']}\n",
        "\n",
        "def collate_fn(batch):\n",
        "    features = [item['features'] for item in batch]\n",
        "    pos = [item['pos'] for item in batch]\n",
        "    return {'features': torch.stack(features), 'pos': pos}\n",
        "\n",
        "def calculate_feature_stds(parquet_dir):\n",
        "    spark = SparkSession.builder.getOrCreate()\n",
        "    df = spark.read.parquet(parquet_dir)\n",
        "    features_rdd = df.select('features').rdd.map(lambda row: np.array(row['features'].toArray()))\n",
        "    all_features = np.vstack(features_rdd.collect())\n",
        "    feature_stds = all_features.std(axis=0)\n",
        "    feature_means = all_features.mean(axis=0)\n",
        "    return feature_stds, feature_means\n",
        "\n",
        "def create_dataloader(parquet_dir, batch_size, shuffle):\n",
        "    feature_stds, feature_means = calculate_feature_stds(parquet_dir)\n",
        "    dataset = ParquetDataset(parquet_dir, feature_stds, feature_means)\n",
        "    return DataLoader(dataset, batch_size=batch_size, shuffle=shuffle, collate_fn=collate_fn)\n",
        "'''\n",
        "\n",
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import numpy as np\n",
        "\n",
        "class NumpyDataset(Dataset):\n",
        "    def __init__(self, numpy_array, global_std, global_mean):\n",
        "        self.data = numpy_array\n",
        "        self.global_std = 1\n",
        "        self.global_mean = 0\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        features = torch.Tensor(self.data[idx])\n",
        "        normalized_features = (features - self.global_mean) / self.global_std\n",
        "        return {'features': normalized_features, 'pos': idx}\n",
        "\n",
        "def calculate_global_mean_std(numpy_array):\n",
        "    global_std = numpy_array.std(axis=0)\n",
        "    global_mean = numpy_array.mean(axis=0)\n",
        "    return global_std, global_mean\n",
        "\n",
        "def create_dataloader(dira, batch_size, shuffle):\n",
        "    numpy_array = np.load(\"/content/drive/My Drive/MSI-Segmentation/dane/nowe_dane/numpy_\" + convolve +'.npy')\n",
        "    global_std, global_mean = calculate_global_mean_std(numpy_array)\n",
        "    dataset = NumpyDataset(numpy_array, global_std, global_mean)\n",
        "    return DataLoader(dataset, batch_size=batch_size, shuffle=shuffle, drop_last=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Nk-OKL2I3Ggj",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d3eac26d-8c8d-4fc7-ecf0-f7b0bf6d71ed"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 0: loss: 33.77487987279892\n",
            "Epoch 1: loss: 30.165886998176575\n",
            "Epoch 2: loss: 25.888874262571335\n",
            "Epoch 3: loss: 23.200800389051437\n",
            "Epoch 4: loss: 21.594064027071\n",
            "Epoch 5: loss: 19.91277366876602\n",
            "Epoch 6: loss: 18.69674338400364\n",
            "Epoch 7: loss: 17.922646805644035\n",
            "Epoch 8: loss: 17.32943093776703\n",
            "Epoch 9: loss: 16.77104239165783\n",
            "Epoch 10: loss: 16.335638850927353\n",
            "Epoch 11: loss: 15.993371203541756\n",
            "Epoch 12: loss: 15.700620979070663\n",
            "Epoch 13: loss: 15.491177514195442\n",
            "Epoch 14: loss: 15.337759032845497\n",
            "Epoch 15: loss: 15.213954344391823\n",
            "Epoch 16: loss: 15.139802530407906\n",
            "Epoch 17: loss: 15.080673664808273\n",
            "Epoch 18: loss: 15.056514337658882\n",
            "Epoch 19: loss: 15.039881765842438\n",
            "Epoch 20: loss: 15.032120540738106\n",
            "Epoch 21: loss: 15.035974204540253\n",
            "Epoch 22: loss: 15.036480367183685\n",
            "Epoch 23: loss: 15.01811358332634\n",
            "Epoch 24: loss: 14.98993493616581\n",
            "Epoch 25: loss: 14.939808800816536\n",
            "Epoch 26: loss: 14.883928179740906\n",
            "Epoch 27: loss: 14.816321805119514\n",
            "Epoch 28: loss: 14.765039548277855\n",
            "Epoch 29: loss: 14.68217334151268\n",
            "Epoch 30: loss: 14.583449572324753\n",
            "Epoch 31: loss: 14.443836376070976\n",
            "Epoch 32: loss: 14.501472279429436\n",
            "Epoch 33: loss: 14.426298454403877\n",
            "Epoch 34: loss: 14.168570816516876\n",
            "Epoch 35: loss: 14.181465968489647\n",
            "Epoch 36: loss: 14.10165037214756\n",
            "Epoch 37: loss: 13.989507332444191\n",
            "Epoch 38: loss: 13.790457591414452\n",
            "Epoch 39: loss: 13.758525803685188\n",
            "Epoch 40: loss: 13.777363151311874\n",
            "Epoch 41: loss: 13.672388598322868\n",
            "Epoch 42: loss: 13.57916447520256\n",
            "Epoch 43: loss: 13.535531848669052\n",
            "Epoch 44: loss: 13.371708273887634\n",
            "Epoch 45: loss: 13.259099513292313\n",
            "Epoch 46: loss: 13.246648728847504\n",
            "Epoch 47: loss: 13.163347318768501\n",
            "Epoch 48: loss: 13.11183388531208\n",
            "Epoch 49: loss: 13.049982607364655\n",
            "Epoch 50: loss: 12.958397433161736\n",
            "Epoch 51: loss: 12.917259693145752\n",
            "Epoch 52: loss: 12.85578565299511\n",
            "Epoch 53: loss: 12.831009402871132\n",
            "Epoch 54: loss: 12.790148630738258\n",
            "Epoch 55: loss: 12.761668086051941\n",
            "Epoch 56: loss: 12.746403068304062\n",
            "Epoch 57: loss: 12.736299365758896\n",
            "Epoch 58: loss: 12.724597439169884\n",
            "Epoch 59: loss: 12.722641363739967\n",
            "Epoch 60: loss: 12.719778910279274\n",
            "Epoch 61: loss: 12.722147315740585\n",
            "Epoch 62: loss: 12.723140522837639\n",
            "Epoch 63: loss: 12.725170210003853\n",
            "Epoch 64: loss: 12.727073326706886\n",
            "Epoch 65: loss: 12.735033318400383\n",
            "Epoch 66: loss: 12.744904592633247\n",
            "Epoch 67: loss: 12.776197329163551\n",
            "Epoch 68: loss: 12.781691133975983\n",
            "Epoch 69: loss: 12.816891297698021\n",
            "Epoch 70: loss: 12.875980004668236\n",
            "Epoch 71: loss: 12.928295969963074\n",
            "Epoch 72: loss: 13.034066081047058\n",
            "Epoch 73: loss: 13.006497159600258\n",
            "Epoch 74: loss: 12.953490227460861\n",
            "Epoch 75: loss: 13.19614090025425\n"
          ]
        }
      ],
      "source": [
        "# Example usage\n",
        "dataloader = create_dataloader(parquet_dir, hparams_CLR.batch_size, True)\n",
        "\n",
        "# Train the model\n",
        "model_CLR, total_losses, epoch_losses_list = fit_clr(hparams_CLR, output_folder, dataloader)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zDUqKsWjVXx6"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "\n",
        "def plot_epoch_losses(epoch_losses_list, title=\"Training Losses\", xlabel=\"Epochs\", ylabel=\"Loss\"):\n",
        "    \"\"\"\n",
        "    Plots the losses for each epoch from a list of dictionaries.\n",
        "\n",
        "    Parameters:\n",
        "    - epoch_losses_list (list of dict): The list containing dictionaries of loss values for each epoch.\n",
        "    - title (str): The title of the plot.\n",
        "    - xlabel (str): The label for the x-axis.\n",
        "    - ylabel (str): The label for the y-axis.\n",
        "    \"\"\"\n",
        "\n",
        "    # Initialize dictionaries to hold losses by type\n",
        "    losses_by_type = {\n",
        "        'contrastive_loss': [],\n",
        "        'std_loss': [],\n",
        "        'mean_loss': [],\n",
        "        'decoder_loss': [],\n",
        "        'total_loss': []\n",
        "    }\n",
        "\n",
        "    # Extract losses for each epoch\n",
        "    for epoch_losses in epoch_losses_list:\n",
        "        for key in losses_by_type.keys():\n",
        "            losses_by_type[key].append(epoch_losses[key])\n",
        "\n",
        "    plt.figure(figsize=(20, 5))  # Adjust the width to accommodate more subplots\n",
        "\n",
        "    # Plot each type of loss in its own subplot\n",
        "    for idx, (key, losses) in enumerate(losses_by_type.items(), 1):\n",
        "        plt.subplot(1, 5, idx)  # 1 row, 5 columns, current plot index\n",
        "        plt.plot(losses, label=key)\n",
        "        plt.title(key)\n",
        "        plt.xlabel(xlabel)\n",
        "        plt.ylabel(ylabel)\n",
        "        plt.grid(True)\n",
        "\n",
        "    plt.suptitle(title)  # Super title for all subplots\n",
        "    plt.tight_layout(rect=[0, 0, 1, 0.95])  # Adjust layout to fit title\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "plot_epoch_losses(epoch_losses_list)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CzqIQA5pUbMc"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def run_thru_NN(model, parquet_dir):\n",
        "    dataloader = iter(create_dataloader(parquet_dir, batch_size=1, shuffle=False))\n",
        "    model.eval()\n",
        "\n",
        "    collected_output_vectors = []\n",
        "\n",
        "    au = 0\n",
        "\n",
        "    X = next(dataloader, None)\n",
        "    while X is not None:\n",
        "        X_tensor = torch.Tensor(X['features']).to(model.device)\n",
        "        with torch.no_grad():\n",
        "            output_batch, decoder_output_batch = model(X_tensor)\n",
        "\n",
        "        input_vector = X_tensor.detach().cpu().numpy().squeeze()\n",
        "        output_vector = output_batch.detach().cpu().numpy().squeeze()\n",
        "        decoder_output = decoder_output_batch.detach().cpu().numpy().squeeze()\n",
        "\n",
        "        collected_output_vectors.append(output_vector)\n",
        "        au += 1\n",
        "        if au in [1, 2, 715, 716, 925, 926, 1599, 1600]:\n",
        "            print(X['pos'][0])\n",
        "            plot_timeseries(input_vector, output_vector, decoder_output, noise(X_tensor.detach().cpu()).numpy().squeeze())\n",
        "\n",
        "        X = next(dataloader, None)\n",
        "\n",
        "\n",
        "    # Convert collected output vectors to a numpy array and save\n",
        "    collected_output_vectors = np.array(collected_output_vectors)\n",
        "    output_file_path = os.path.join(output_folder, 'CLR_feat.npy')\n",
        "    np.save(output_file_path, collected_output_vectors)\n",
        "    print(f\"Output vectors saved to {output_file_path}\")\n",
        "\n",
        "\n",
        "def plot_timeseries(input_vector, output_vector, decoder_output, noised):\n",
        "    fig, axes = plt.subplots(1, 4, figsize=(18, 6))\n",
        "\n",
        "    # Plot input vector as time series\n",
        "    axes[0].plot(input_vector, 'r-', alpha=0.5)\n",
        "    axes[0].set_title('Input Vector Time Series')\n",
        "    axes[0].legend(['Input Vector'])\n",
        "\n",
        "    # Plot output vector as time series\n",
        "    axes[1].plot(output_vector, 'g-', alpha=0.5)\n",
        "    axes[1].set_title('Output Vector Time Series')\n",
        "    axes[1].legend(['Output Vector'])\n",
        "\n",
        "    # Plot decoder output vector as time series\n",
        "    axes[2].plot(decoder_output, 'b-', alpha=0.5)\n",
        "    axes[2].set_title('Decoder Output Vector Time Series')\n",
        "    axes[2].legend(['Decoder Output'])\n",
        "\n",
        "    # Plot noised input vector as time series\n",
        "    axes[3].plot(noised, 'b-', alpha=0.5)\n",
        "    axes[3].set_title('Noised Vector Time Series')\n",
        "    axes[3].legend(['Noised Output'])\n",
        "\n",
        "    plt.show()\n",
        "\n",
        "run_thru_NN(model_CLR, parquet_dir)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SmHWA5NyPPdI"
      },
      "outputs": [],
      "source": [
        "# Your code block here\n",
        "\n",
        "end_time = time.time()\n",
        "execution_time = end_time - start_time\n",
        "print(f\"Execution time: {execution_time} seconds\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rznBjpfqLpKa"
      },
      "outputs": [],
      "source": [
        "for key, value in model_CLR.state_dict().items():\n",
        "    print(f\"{key}: {value.shape}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uza8x3Pyd9Dd"
      },
      "outputs": [],
      "source": [
        "if False:\n",
        "    checkpoint_path = \"/content/drive/My Drive/MSI-Segmentation/results/sztuczne_dane_08-11-2023-10-51_conv_True/model_CLR.ckpt\"\n",
        "    checkpoint = torch.load(checkpoint_path, map_location=torch.device('cpu'))\n",
        "    model_CLR.load_state_dict(checkpoint)  # Load the state dict\n",
        "\n",
        "    run_thru_NN(model_CLR, parquet_dir)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QMXga1_rjJSG"
      },
      "outputs": [],
      "source": [
        "if False:\n",
        "    checkpoint_path = \"/content/drive/My Drive/MSI-Segmentation/results/sztuczne_dane_25-07-2024-20-25_conv_True/model_CLR.ckpt\"\n",
        "    checkpoint = torch.load(checkpoint_path, map_location=torch.device('cpu'))\n",
        "    model_CLR.load_state_dict(checkpoint)  # Load the state dict\n",
        "\n",
        "    run_thru_NN(model_CLR, parquet_dir)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}